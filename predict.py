import torch
import os
from cog import BasePredictor, Input, Path
from transformers import AutoModelForCausalLM, AutoTokenizer
# ğŸŒŸğŸŒŸğŸŒŸ (ì‹ ê·œ) ëŸ°íƒ€ì„ì— snapshot_downloadë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ import ğŸŒŸğŸŒŸğŸŒŸ
from huggingface_hub import snapshot_download

# (ì£¼ì˜) cog.yamlì—ì„œ ìºì‹œí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì´ ê²½ë¡œëŠ” ì´ì œ ë¡œì»¬ ê²½ë¡œê°€ ì•„ë‹™ë‹ˆë‹¤.
MODEL_ID = "black-forest-labs/FLUX.1-dev"
LORA_PATH = "/src/loras/Flux_Capybara_v1.safetensors"

class Predictor(BasePredictor):
    def setup(self):
        """ğŸŒŸğŸŒŸğŸŒŸ (ìˆ˜ì •) ëŸ°íƒ€ì„ì— ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œë“œí•©ë‹ˆë‹¤ ğŸŒŸğŸŒŸğŸŒŸ"""
        print("Booting... Attempting to download model (this may take a while)...")
        
        # 1. 'push.yml'ì˜ env: ì—ì„œ ì „ë‹¬ëœ HF_TOKENì„ ì½ìŠµë‹ˆë‹¤.
        huggingface_token = os.environ.get("HF_TOKEN")
        
        if not huggingface_token:
            print("WARNING: HF_TOKEN environment variable not set. Download may fail.")
        
        # 2. ëŸ°íƒ€ì„ì— ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (22GB)
        # (ì´ê²ƒì´ íƒ€ì„ì•„ì›ƒë  ìˆ˜ ìˆì§€ë§Œ, ìœ ì¼í•œ ë°©ë²•ì…ë‹ˆë‹¤.)
        downloaded_model_path = snapshot_download(
            repo_id=MODEL_ID,
            token=huggingface_token,
            cache_dir="/root/.cache/huggingface"
            # local_dir="/src/models" # ìºì‹œë¥¼ ì‚¬ìš©í•˜ë„ë¡ local_dir ì£¼ì„ ì²˜ë¦¬
        )
        print("Model download complete.")

        # 3. ë¡œì»¬ ìºì‹œ ê²½ë¡œì—ì„œ ëª¨ë¸ ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            downloaded_model_path
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            downloaded_model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # 4. LoRA ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)
        # (ì´ ë¶€ë¶„ì€ ì‚¬ìš©ìë‹˜ì˜ ì‹¤ì œ LoRA ë¡œë“œ ì½”ë“œë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤)
        # self.model.load_adapter(LORA_PATH) 
        # print(f"LoRA loaded from {LORA_PATH}")

        print("Model loaded successfully. Booting complete.")

    def predict(
        self,
        prompt: str = Input(description="Inquiry for the model."),
        max_new_tokens: int = Input(description="Maximum number of tokens to generate.", default=128, ge=1, le=2048),
        temperature: float = Input(description="Creativity of the generation.", default=0.9, ge=0.01, le=1.0),
        top_p: float = Input(description="Probability mass of tokens to consider.", default=0.9, ge=0.0, le=1.0),
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            output_tokens = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
            )
        
        output_text = self.tokenizer.decode(
            output_tokens[0], 
            skip_special_tokens=True
        )
        
        return output_text.replace(prompt, "", 1).strip()