import torch
import os
from cog import BasePredictor, Input, Path
# ğŸŒŸ (ìˆ˜ì •) Diffusersì˜ FluxPipelineì„ import
from diffusers import FluxPipeline
# ğŸŒŸ (ì‹ ê·œ) ëŸ°íƒ€ì„ì— snapshot_downloadë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ import
from huggingface_hub import snapshot_download

MODEL_ID = "black-forest-labs/FLUX.1-dev"
LORA_PATH = "/src/loras/Flux_Capybara_v1.safetensors"

class Predictor(BasePredictor):
    def setup(self):
        """ğŸŒŸ (ìˆ˜ì •) ëŸ°íƒ€ì„ì— FluxPipelineì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œë“œí•©ë‹ˆë‹¤ ğŸŒŸ"""
        print("Booting... Attempting to download FLUX.1-dev pipeline (this may take a while)...")
        
        # 1. 'push.yml'ì˜ env: ì—ì„œ ì „ë‹¬ëœ HF_TOKENì„ ì½ìŠµë‹ˆë‹¤.
        huggingface_token = os.environ.get("HF_TOKEN")
        
        if not huggingface_token:
            print("WARNING: HF_TOKEN environment variable not set. Download may fail.")
        
        # 2. ëŸ°íƒ€ì„ì— Gated Model ë‹¤ìš´ë¡œë“œ (22GB)
        downloaded_model_path = snapshot_download(
            repo_id=MODEL_ID,
            token=huggingface_token,
            cache_dir="/root/.cache/huggingface"
        )
        print("Model download complete.")

        # 3. ë¡œì»¬ ìºì‹œ ê²½ë¡œì—ì„œ ëª¨ë¸ ë¡œë“œ
        self.pipe = FluxPipeline.from_pretrained(
            downloaded_model_path,
            torch_dtype=torch.bfloat16
        )
        
        # 4. VRAM ì ˆì•½ì„ ìœ„í•´ CPU ì˜¤í”„ë¡œë“œ
        self.pipe.enable_model_cpu_offload()
        
        # 5. LoRA ë¡œë“œ
        # (ì´ ë¶€ë¶„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ LoRAë¥¼ ì ìš©í•©ë‹ˆë‹¤)
        self.pipe.load_lora_weights(LORA_PATH)
        print(f"LoRA loaded from {LORA_PATH}")

        print("FluxPipeline loaded successfully. Booting complete.")

    def predict(
        self,
        prompt: str = Input(description="Prompt for the model."),
        height: int = Input(description="Height of the image.", default=1024),
        width: int = Input(description="Width of the image.", default=1024),
        num_inference_steps: int = Input(description="Number of inference steps.", default=50),
        guidance_scale: float = Input(description="Guidance scale.", default=3.5)
    ) -> Path: # ğŸŒŸ (ìˆ˜ì •) ë°˜í™˜ íƒ€ì…ì´ Path(íŒŒì¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.
        """í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        image = self.pipe(
            prompt=prompt,
            height=height,
            width=width,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            generator=torch.Generator("cpu").manual_seed(0)
        ).images[0]
        
        output_path = "/tmp/output.png"
        image.save(output_path)
        return Path(output_path)