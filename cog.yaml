# cog.yaml
build:
  gpu: true
  cuda: "12.1"
  python_version: "3.11" 
  python_packages:
    - "torch==2.1.2"
    - "diffusers==0.29.0"
    - "transformers==4.41.2"
    - "accelerate==0.30.1"
    - "huggingface-hub"  # 모델 다운로드를 위해 추가

  run:
    # 1. Hugging Face CLI를 사용하여 대용량 모델을 빌드 이미지 내부 경로(/src/models)에 다운로드
    # REPLICATE_SECRET_HF_TOKEN은 cog push 명령에서 --build-arg로 전달되어야 합니다.
    # 주의: 'meta-llama/Llama-2-7b-chat-hf'를 실제 Flux Dev 모델 ID로 반드시 교체하세요.
    - huggingface-cli download --local-dir /src/models --repo-id meta-llama/Llama-2-7b-chat-hf --token ${REPLICATE_SECRET_HF_TOKEN} --cache-dir /root/.cache/huggingface
    
    # 2. LoRA 파일을 다운로드합니다.
    - "mkdir -p /src/loras"
    - "curl -L -o /src/loras/Flux_Capybara_v1.safetensors https://github.com/SanghyeonpJeong/comfyUI_Flux_Capybara_Loras/raw/main/Flux_Capybara_v1.safetensors"

predict: "predict.py:Predictor"