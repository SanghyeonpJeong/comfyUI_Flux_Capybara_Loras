# cog.yaml
build:
  gpu: true
  cuda: "12.1"
  python_version: "3.11" 
  python_packages:
    - "torch==2.1.2"
    - "diffusers==0.29.0"
    - "transformers==4.41.2"
    - "accelerate==0.30.1"
    - "huggingface-hub"  # ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ í•„ìš”

  run:
    # ğŸŒŸğŸŒŸğŸŒŸ í•´ê²°ì±… 1: PATH í™˜ê²½ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (pyenv ë°”ì´ë„ˆë¦¬ ê²½ë¡œ ì¶”ê°€) ğŸŒŸğŸŒŸğŸŒŸ
    - export PATH="$(pyenv prefix)/bin:$PATH"

    # ğŸŒŸğŸŒŸğŸŒŸ í•´ê²°ì±… 2 (ë” í™•ì‹¤í•¨): Python ëª¨ë“ˆë¡œ ì‹¤í–‰ (huggingface-clië¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ) ğŸŒŸğŸŒŸğŸŒŸ
    # ì£¼ì˜: 'meta-llama/Llama-2-7b-chat-hf'ë¥¼ ì‹¤ì œ Flux Dev ëª¨ë¸ IDë¡œ ë°˜ë“œì‹œ êµì²´í•˜ì„¸ìš”.
    - python -m huggingface_hub download --local-dir /src/models --repo-id meta-llama/Llama-2-7b-chat-hf --token ${REPLICATE_SECRET_HF_TOKEN} --cache-dir /root/.cache/huggingface
    
    # 3. LoRA íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    - "mkdir -p /src/loras"
    - "curl -L -o /src/loras/Flux_Capybara_v1.safetensors https://github.com/SanghyeonpJeong/comfyUI_Flux_Capybara_Loras/raw/main/Flux_Capybara_v1.safetensors"

predict: "predict.py:Predictor"