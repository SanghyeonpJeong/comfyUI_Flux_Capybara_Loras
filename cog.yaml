build:
  gpu: true
  cuda: "12.1"
  python_version: "3.11" 
  python_packages:
    - "torch==2.1.2"
    - "diffusers==0.29.0"
    - "transformers==4.41.2"
    - "accelerate==0.30.1"
    - "huggingface-hub[cli]" 

  run:
    # ğŸŒŸğŸŒŸğŸŒŸ 1. (ìˆ˜ì •) /run/secrets/my_hf_secret ì—ì„œ í† í°ì„ ì½ìŠµë‹ˆë‹¤. ğŸŒŸğŸŒŸğŸŒŸ
    # (push.ymlì˜ --secret id=... ì™€ ì´ë¦„ì´ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤)
    # ğŸŒŸğŸŒŸğŸŒŸ 2. (ìˆ˜ì •) ëª¨ë“  ëª…ë ¹ì„ &&ë¡œ ì—°ê²°í•˜ì—¬ 'ì¤„ ë°”ê¿ˆ' ì˜¤ë¥˜ë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ğŸŒŸğŸŒŸğŸŒŸ
    - "echo 'Reading token from secret env...' && HF_TOKEN_VALUE=$(cat /run/secrets/my_hf_secret) && python -c \"import os; from huggingface_hub import snapshot_download; print('Attempting to download model...'); snapshot_download(repo_id=\\\"black-forest-labs/FLUX.1-dev\\\", local_dir=\\\"/src/models\\\", token='${HF_TOKEN_VALUE}', cache_dir=\\\"/root/.cache/huggingface\\\")\""
    
    # 2. LoRA íŒŒì¼ ë‹¤ìš´ë¡œë“œ (URL ê²½ë¡œ ìˆ˜ì •)
    - "mkdir -p /src/loras"
    - "curl -L -o /src/loras/Flux_Capybara_v1.safetensors https://github.com/SanghyeonpJeong/comfyUI_Flux_Capybara_Loras/raw/main/Flux_Capybara_v1.safetensors"
    
    # Gitì´ ë³€ê²½ ì‚¬í•­ì„ ì¸ì‹í•˜ë„ë¡ ê°•ì œ ì£¼ì„ ì¶”ê°€
    # Force update v18

predict: "predict.py:Predictor"