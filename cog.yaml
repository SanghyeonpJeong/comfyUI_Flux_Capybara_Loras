build:
  gpu: true
  cuda: "12.1"
  python_version: "3.11" 
  python_packages:
    - "torch==2.1.2"
    - "diffusers==0.29.0"
    - "transformers==4.41.2"
    - "accelerate==0.30.1"
    - "huggingface-hub[cli]" 

  run:
    # ğŸŒŸğŸŒŸğŸŒŸ 1. ë¹Œë“œ ì¸ì(HF_TOKEN_BUILD_ARG)ë¥¼ í™˜ê²½ ë³€ìˆ˜(HF_TOKEN)ë¡œ ì„¤ì • ğŸŒŸğŸŒŸğŸŒŸ
    # ğŸŒŸğŸŒŸğŸŒŸ 2. Python ìŠ¤í¬ë¦½íŠ¸ê°€ ì´ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì½ë„ë¡ í•¨ ğŸŒŸğŸŒŸğŸŒŸ
    # | (íŒŒì´í”„)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì¤„ ëª…ë ¹ì–´ë¥¼ í•˜ë‚˜ì˜ RUN ìŠ¤í…ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    - |
      export HF_TOKEN=${HF_TOKEN_BUILD_ARG}
      python -c "import os; from huggingface_hub import snapshot_download; \
      print('Attempting to download model... Token starts with:', os.environ.get('HF_TOKEN')[:4]); \
      snapshot_download( \
        repo_id=\"black-forest-labs/FLUX.1-dev\", \
        local_dir=\"/src/models\", \
        token=os.environ.get(\"HF_TOKEN\"), \
        cache_dir=\"/root/.cache/huggingface\" \
      )"
    
    # 2. LoRA íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    - "mkdir -p /src/loras"
    - "curl -L -o /src/loras/Flux_Capybara_v1.safetensors https://github.com/SanghyeonpJeong/comfyUI_Flux_Capybara_Loras/raw/main/Flux_Capybara_v1.safetensors"
    
    # Gitì´ ë³€ê²½ ì‚¬í•­ì„ ì¸ì‹í•˜ë„ë¡ ê°•ì œ ì£¼ì„ ì¶”ê°€
    # Force update v15

predict: "predict.py:Predictor"