build:
  gpu: true
  cuda: "12.1"
  python_version: "3.11" 
  python_packages:
    - "torch==2.1.2"
    - "diffusers==0.29.0"
    - "transformers==4.41.2"
    - "accelerate==0.30.1"
    # ğŸŒŸ 1. 'snapshot_download' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ 'huggingface-hub' ì„¤ì¹˜
    - "huggingface-hub[cli]" 

  run:
    # ğŸŒŸğŸŒŸğŸŒŸ 2. í•µì‹¬ ìˆ˜ì •: ë¹Œë“œ ì¸ì(${...})ë¥¼ í™˜ê²½ ë³€ìˆ˜(REPLICATE_SECRET_HF_TOKEN=)ë¡œ ì£¼ì… ğŸŒŸğŸŒŸğŸŒŸ
    # Pythonì˜ os.environ.get()ì´ í† í°ì„ ì½ì„ ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
    # 'meta-llama/Llama-2-7b-chat-hf'ë¥¼ ì‹¤ì œ Flux Dev ëª¨ë¸ IDë¡œ ë°˜ë“œì‹œ êµì²´í•˜ì„¸ìš”.
    - 'REPLICATE_SECRET_HF_TOKEN=${REPLICATE_SECRET_HF_TOKEN} python -c "import os; from huggingface_hub import snapshot_download; snapshot_download(repo_id=\"black-forest-labs/FLUX.1-dev\", local_dir=\"/src/models\", token=os.environ.get(\"REPLICATE_SECRET_HF_TOKEN\"), cache_dir=\"/root/.cache/huggingface\")"'
    
    # 3. LoRA íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    - "mkdir -p /src/loras"
    - "curl -L -o /src/loras/Flux_Capybara_v1.safetensors https://github.com/SanghyeonpJeong/comfyUI_Flux_Capybara_Loras/raw/main/Flux_Capybara_v1.safetensors"
    
    # Gitì´ ë³€ê²½ ì‚¬í•­ì„ ì¸ì‹í•˜ë„ë¡ ê°•ì œ ì£¼ì„ ì¶”ê°€
    # Force update v13

predict: "predict.py:Predictor"
