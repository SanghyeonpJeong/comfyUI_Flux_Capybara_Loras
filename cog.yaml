build:
  gpu: true
  cuda: "12.1"
  python_version: "3.11" 
  python_packages:
    - "torch==2.1.2"
    - "diffusers==0.29.0"
    - "transformers==4.41.2"
    - "accelerate==0.30.1"
    - "huggingface-hub" 

  run:
    # ğŸŒŸğŸŒŸğŸŒŸ í•µì‹¬ ìˆ˜ì •: PATH ì„¤ì •ê³¼ huggingface-cli ì‹¤í–‰ì„ í•˜ë‚˜ì˜ RUN ëª…ë ¹ìœ¼ë¡œ í†µí•© ğŸŒŸğŸŒŸğŸŒŸ
    # &&ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ì‰˜ ì„¸ì…˜ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.
    # ì£¼ì˜: 'meta-llama/Llama-2-7b-chat-hf'ë¥¼ ì‹¤ì œ Flux Dev ëª¨ë¸ IDë¡œ ë°˜ë“œì‹œ êµì²´í•˜ì„¸ìš”.
    - "export PATH=\"$(pyenv prefix)/bin:$PATH\" && huggingface-cli download --local-dir /src/models --repo-id meta-llama/Llama-2-7b-chat-hf --token ${REPLICATE_SECRET_HF_TOKEN} --cache-dir /root/.cache/huggingface"
    
    # 2. LoRA íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì´ê²ƒì€ ë³„ê°œì˜ RUN ëª…ë ¹ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤)
    - "mkdir -p /src/loras"
    - "curl -L -o /src/loras/Flux_Capybara_v1.safetensors https://github.com/SanghyeonpJeong/comfyUI_Flux_Capybara_Loras/raw/main/Flux_Capybara_v1.safetensors"
    
    # ğŸŒŸğŸŒŸğŸŒŸ Gitì´ ë³€ê²½ ì‚¬í•­ì„ ì¸ì‹í•˜ë„ë¡ ê°•ì œ ì£¼ì„ ì¶”ê°€ ğŸŒŸğŸŒŸğŸŒŸ
    # Force update v6

predict: "predict.py:Predictor"